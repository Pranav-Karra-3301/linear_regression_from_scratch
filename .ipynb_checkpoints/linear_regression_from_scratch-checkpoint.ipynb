{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Baakchsu/LinearRegression/master/weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def fit(self, X, Y):\n",
    "        X = np.array(X).reshape(-1,1)\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        x_shape = X.shape\n",
    "        self.parameter_cache = []\n",
    "        num_var = x_shape[1]\n",
    "        self.weight_matrix = np.random.normal(-1, 1, (num_var, 1))\n",
    "        self.intercept = np.random.rand(1)\n",
    "        for i in range(50):\n",
    "            self.dcost_dm = np.sum(np.multiply(((np.matmul(X, self.weight_matrix) + self.intercept) - Y), X)) * 2 / x_shape[0]\n",
    "            self.dcost_dc = np.sum(((np.matmul(X, self.weight_matrix) + self.intercept) - Y)) * 2 / x_shape[0]\n",
    "            self.weight_matrix -= 0.1 * self.dcost_dm\n",
    "            self.intercept -= 0.1 * self.dcost_dc\n",
    "            self.parameter_cache.append(np.array((self.weight_matrix, self.intercept)))\n",
    "        return self.weight_matrix, self.intercept, self.parameter_cache\n",
    "\n",
    "    def predict(self, X):\n",
    "        product = np.matmul(np.array(X).reshape(-1, 1), self.weight_matrix) + self.intercept\n",
    "        return product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation of the Linear Regression Class\n",
    "### Learning Rate:\n",
    "> The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated\n",
    "#### What is a Hyper Parameter?\n",
    ">Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix ‘hyper_’ suggests that they are ‘top-level’ parameters that control the learning process and the model parameters that result from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "### Weights:\n",
    "> Weights are the co-efficients of the equation which you are trying to resolve. Negative weights reduce the value of an output.\n",
    "\n",
    "![Weights and Biases](images/weightsbiases.png)\n",
    "\n",
    "### Biases\n",
    "> Biases, which are constant, are an additional input into the next layer that will always have the value of 1. Bias units are not influenced by the previous layer (they do not have any incoming connections) but they do have outgoing connections with their own weights. The bias unit guarantees that even when all the inputs are zeros there will still be an activation in the neuron.\n",
    "\n",
    "## For now both weights and Biases are set to none because it depends on the features of the input data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
